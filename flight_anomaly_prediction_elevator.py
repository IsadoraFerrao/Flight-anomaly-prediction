# -*- coding: utf-8 -*-
"""flight_anomaly_prediction-elevator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BjDXDW-mmJosnRK5Z6ez_EvdyQaKrQeF
"""

#pip install dill

# Commented out IPython magic to ensure Python compatibility.
# Imports
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import time
import warnings
import dill
import sys
from glob import glob
#from google.colab import drive
from imblearn.under_sampling import RandomUnderSampler
from lightgbm import LGBMClassifier
from sklearn import svm
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import roc_curve, RocCurveDisplay
from sklearn.model_selection import StratifiedKFold
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier


# Google Colaboraty
#from google.colab import drive
#drive.mount('/content/drive') # Mount drive

pd.set_option('mode.chained_assignment', None) # Ignore warning
warnings.filterwarnings('ignore') # Silence scikit-learn warning

# %matplotlib inline

# Random state
RANDOM_STATE = 42

# Data path
data_path = 'C:Users/User/Downloads/Dataset/Dataset/'

# Path and filename DataFrames dictionary
df_file_name = "C:/Users/User/Downloads/Dataset/preprocessed_median-resampling_feature-selection.pkl"

# Functions

def read_data(full_path, timestamp_column="timestamp"):
    # Load the .csv into a DataFrame
    df_tmp = pd.read_csv(full_path)
    # Rename timestamp column
    df_tmp = df_tmp.rename(columns={"%time": timestamp_column})
    # Convert timestamp column to nanoseconds
    df_tmp[timestamp_column] = pd.to_datetime(df_tmp[timestamp_column], unit="ns")
    # DataFrame index is the timestamp column
    df_tmp.set_index(timestamp_column, inplace=True)
    # Return DataFrame after changes
    return df_tmp



def extract_topic_name(flight_name, file_name):
    topic_name = file_name.split(flight_name)
    topic_name = topic_name[1]
    topic_name = topic_name[1:]
    topic_name = topic_name.split(".csv")
    topic_name = topic_name[0]
    # Return topic name after extracted
    return topic_name



def pre_processing_data(data_path, unused_flight_list, unused_topic_list, unused_columns, verbose=False):
    # Used dictionaries and lists
    time_dict = {}
    flight_topic_dict = {}
    topic_list = []
    all_columns = []
    df_dict = {}

    # Iterate over the list of flight names
    for i, flight in enumerate(glob(os.path.join(data_path + "*"))):
        if any(x in flight for x in unused_flight_list):
            continue # If the flight is in the unused list, continue
        
        flight_name = os.path.basename(flight) # Flight name

        if verbose: print(flight_name) # Show flight name
        
        # Check if the flight name is present in the dictionaries below
        # If not, add it as a key
        if flight_name not in time_dict:
            time_dict[flight_name] = []

        if flight_name not in flight_topic_dict:
            flight_topic_dict[flight_name] = []
        
        # Start the Merged DataFrame as None
        df_merged = None
        
        # Iterate over the list of topics
        for k, topic in enumerate(glob(flight + "/*.csv")):
            if any(x in topic for x in unused_topic_list):
                continue # If the topic is in the unused list, continue
            
            file_name = os.path.basename(topic) # Filename
            topic_list.append(file_name) # Add filename in topic list
            # Extract topic name
            topic_name = extract_topic_name(flight_name, file_name)
            # Add topic name to topic dictionary
            flight_topic_dict[flight_name].append(topic_name)
            
            dfx = read_data(topic) # Read data
            # Drop unused columns
            dfx = dfx.drop(unused_columns, axis=1, errors="ignore")
            # Rename existing columns
            new_columns = list(map(lambda x: f"{topic_name}.{x.replace('field.', '')}", dfx.columns))
            # Change column names
            dfx = dfx.set_axis(new_columns, axis=1, inplace=False)
            # Drop all covariance columns
            dfx = dfx.drop(dfx.filter(regex='covariance').columns, axis=1)
            # Resample the dataset to 5Hz frequency 
            dfx = dfx.resample("200ms").median()
            
            if df_merged is None: # Merged DataFrame is none
                df_merged = dfx
            else: # Merge the existing DataFrame with the new one
                df_merged = df_merged.merge(dfx, left_index=True, right_index=True, how="outer")
            
            # Replace NaN values
            df_merged.iloc[0] = df_merged.iloc[0].fillna(0)
            df_merged = df_merged.pad()

            # Add DataFrame columns to column list
            all_columns.append(list(dfx.columns))
            
            # Difference of seconds between samples
            diff_seconds = pd.to_timedelta((dfx.index[-1] - dfx.index[0])).total_seconds()
            # Convert to seconds
            diff_seconds = int(diff_seconds)
            # Add the difference in the time dictionary
            time_dict[flight_name].append(diff_seconds)
            # Drop unused columns from Merged DataFrame
            df_merged = df_merged.drop(unused_columns, axis=1, errors="ignore")
            # Add the Merged DataFrame to the DataFrames dictionary
            df_dict[flight_name] = df_merged

    # Return dictionaries and lists after processing
    return time_dict, flight_topic_dict, topic_list, all_columns, df_dict



def store_flight_data(data_path, df_dict):
    all_data = {} # Store flight information for testing
    cont = 0 # Counter variable

    # Loop over each flight in the DataFrames dictionary
    for flight_name in list(df_dict.keys()):
        if "engine_failure" not in flight_name:
            continue # If there is no elevator failure, continue
        
        # Topic that indicates when it is elevator failure or not
        status_file = flight_name + "-failure_status-engines.csv"

        # Load failure status
        failure_status = read_data(os.path.join(data_path, flight_name, status_file))
        # Resample the data in 200ms
        failure_status = failure_status.resample("200ms").last()
        # Change column name
        failure_status = failure_status.rename(columns={"field.data": "failure_status"})
        
        df = df_dict[flight_name] # Load DataFrame of a flight name
        # Add failure status as DataFrame column
        df = df.merge(failure_status, left_index=True, right_index=True, how="outer")
        # Replace NaN values
        df.iloc[0] = df.iloc[0].fillna(0)
        df = df.pad()
        # Convert failure status column to int
        df["failure_status"] = df["failure_status"].astype(int)

        # Save current DataFrame information
        cont += 1 # count = count + 1
        data_name = "df_"+str(cont) # DataFrame name
        all_data[data_name] = df # Add DataFrame to dictionary

    # Return stored flight data for tests
    return all_data



def plot_columns_by_timestamp(df):
    axes = df.plot(subplots=True, figsize=(18, 30)); # Plot settings
    idx = df["failure_status"].ne(0).idxmax()

    # Loop over all axis objects and create a vertical line in each of them and set legend location
    for ax in axes:
        ax.axvline(idx, color="red", linestyle="--")
        ax.legend(loc='upper center')

    plt.show() # Plot show



def split_X_y(all_data, balance_data=False):
    # X and y of each DataFrame
    all_X = []
    all_y = []

    # Loop over each flight across all data
    for flight in all_data.keys():
        # Get data from a flight
        df = all_data[flight]

        # Split of the data in X and y
        X = df.drop(["failure_status"], axis=1)
        y = df["failure_status"].copy()
        y = y.values

        # Discretization of classes to be predicted
        y[y>0] = 1
        y[y<0] = 0

        if balance_data: # Apply Random Undersampling (RUS)
            X, y = RandomUnderSampler(random_state=RANDOM_STATE).fit_resample(X, y)
        
        # Save data for all flights
        all_X.append(X)
        all_y.append(y)

    # Return all X and y
    return all_X, all_y



def apply_k_fold(all_X, all_y, k=10, verbose=False):
    all_folds = [] # Stores partitions for each flight

    for i in range(len(all_X)): # Loop over each flight
        # X and y of the flight
        X = all_X[i]
        y = all_y[i]
        kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=RANDOM_STATE)
        # K-Fold for flight
        kf.get_n_splits(X)
        # Save K-Fold for each flight
        all_folds.append(kf)

        if verbose: print(f'Data {i + 1:02d} - {kf}')

    all_train_test_data = [] # Save train and test data

    for i in range(len(all_X)): # Loop over each flight
        X_train, y_train, X_test, y_test = [], [], [], []

        # X and y of the flight
        X = all_X[i]
        y = all_y[i]
        
        # Loop over each fold
        for j, (train_index, test_index) in enumerate(kf.split(X, y)):
            if verbose: print(f"Data {i + 1:02d}, Fold: {j + 1:02d}")
            # Add over each list
            X_train.append(X.iloc[train_index])
            y_train.append(y[train_index])
            X_test.append(X.iloc[test_index])
            y_test.append(y[test_index])
            # Save individual flight information
            all_train_test_data.append([X_train, y_train, X_test, y_test])

    # Return data after applying K-Fold
    return all_train_test_data


def report_classification(all_train_test_data, clf, k=10):
    total_score = [] # Save the score of all flights
    predicted_class = [] # Save each y_pred
    original_class = [] # Save each y_true

    t0 = time.perf_counter() # Start time

    for idx, flight in enumerate(all_data.keys()): # Loop over each flight
        X_train = all_train_test_data[idx][0]
        y_train = all_train_test_data[idx][1]
        X_test = all_train_test_data[idx][2]
        y_test = all_train_test_data[idx][3]

        scores = [] # Save scores for each fold for the current flight

        for i in range(k): # Loop over each fold
            clf.fit(X_train[i], y_train[i]) # Train

            # Predict to generate the confusion matrix         
            y_pred = clf.predict(X_test[i])
            predicted_class.extend(y_pred)
            original_class.extend(y_test[i])
            # Return the score    
            score = round(clf.score(X_test[i], y_test[i]), 5)
            scores.append(score) # Add score to scores list

        # Average scores on the flight
        total_score.append(round(np.mean(np.array(scores)), 5))

    t1 = time.perf_counter() # Final time
    result_mean = round(np.mean(np.array(total_score)), 5) # Mean score

    if len(total_score) > 0:
        result_min  = round(np.min(np.array(total_score)), 5) # Min score
    else:
        result_min = np.nan

    result_max  = round(np.max(np.array(total_score)), 5) # Max score
    print(f"All flights: {total_score}")
    print(f"\nMean: {result_mean} - Min: {result_min} - Max: {result_max}")
    print(f"\nTime: {t1 - t0:.5f} seconds")
    # Classification report
    print('\n\n' + classification_report(original_class, predicted_class, target_names=['Normal', 'Failure'], digits=5))
    # Plot confusion matrix
    plot_confusion_matrix(original_class, predicted_class, ['Normal', 'Failure'])
    
# Main

# List of unused flights
unused_flight_list = ["no_ground_truth",]

# List of unused topics
unused_topic_list = ["diagnostics", "emergency_responder-traj_file",
                    "failure_status", "global_position",
                    "local_position", "mavctrl-path_dev",
                    "mavctrl-rpy", "mavlink",
                    "mavros-battery", "mavros-imu-data_raw",
                    "mavros-imu-mag", "mavros-mission-reached",
                    "mavros-rc", "mavros-state",
                    "mavros-time_reference", "setpoint_raw"]

# List of unused columns
unused_columns = ["field.header.seq", "field.header.stamp", "field.header.frame_id", 
                "field.commanded", "field.variance", "field.twist.angular.x",
                "field.twist.angular.y", "field.twist.angular.z",
                "field.coordinate_frame", "mavros-nav_info-airspeed.measured",
                "mavros-vfr_hud.airspeed", "mavros-vfr_hud.throttle",
                "mavros-vfr_hud.altitude", "mavros-imu-atm_pressure.fluid_pressure",
                "mavros-imu-atm_pressure.fluid_pressure", "mavros-imu-data.angular_velocity.z",
                "mavros-imu-data.linear_acceleration.z", "mavros-imu-data.orientation.w",
                "mavros-imu-data.orientation.z", "mavros-imu-data.orientation.z",
                "mavros-imu-temperature.temperature", "mavros-nav_info-airspeed.measured",
                "mavros-nav_info-velocity.des_x", "mavros-nav_info-velocity.des_y",
                "mavros-nav_info-velocity.des_z", "mavros-nav_info-velocity.meas_x",
                "mavros-nav_info-velocity.meas_z", "mavros-nav_info-velocity.meas_z",
                "mavros-vfr_hud.airspeed", "mavros-vfr_hud.altitude",
                "mavros-vfr_hud.throttle", "mavros-wind_estimation.twist.linear.y",
                "mavros-wind_estimation.twist.linear.z", "mavros-vfr_hud.throttle",
                "mavros-vfr_hud.groundspeed"]

_, _, _, _, df_dict = pre_processing_data(data_path, unused_flight_list, unused_topic_list, unused_columns)

# Save DataFrames dictionary
with open(os.path.join("data", df_file_name), "wb") as f:
    dill.dump(df_dict, f)
    
# Load DataFrames dictionary
with open(os.path.join("data", df_file_name), 'rb') as f:
    df_dict = dill.load(f)
    
# Data dictionary for machine learning tests
all_data = store_flight_data(data_path, df_dict)

# Split all data into X and y
all_X, all_y = split_X_y(all_data, balance_data=True)

# Apply Stratified K-Fold to data
all_train_test_data = apply_k_fold(all_X, all_y)


# TRAINING
clf = svm.SVC(random_state=RANDOM_STATE)

# Nome do arquivo de saída
output_file = "resultado_classificacao.txt"

# Abre o arquivo em modo de escrita
with open(output_file, "w") as file:

    # Redireciona a saída padrão para o arquivo
    sys.stdout = file

    # Chama a função "report_classification" e imprime o resultado no arquivo
    report_classification(all_train_test_data, clf)

    # Retorna a saída padrão para a tela
    sys.stdout = sys.__stdout__


# TRAINING
clf = svm.SVC(random_state=RANDOM_STATE)

# Nome do arquivo de saída
output_file = "resultado_classificacao.txt"

# Abre o arquivo em modo de escrita
with open(output_file, "w") as file:

    # Redireciona a saída padrão para o arquivo
    sys.stdout = file

    # Chama a função "report_classification" e imprime o resultado no arquivo
    report_classification(all_train_test_data, clf)

    # Retorna a saída padrão para a tela
    sys.stdout = sys.__stdout__